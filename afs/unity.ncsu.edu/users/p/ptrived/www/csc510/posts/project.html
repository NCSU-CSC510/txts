<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>Class Project</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="../img/posty.css" type="text/css" />
</head>
<body>
<b><font size="+3">Software Engineering, cs510, Spring 2015</font></b>
<div id="navcontainer">
<img width=300 align=right src="http://www.cvm.ncsu.edu/ess/emd/cvm_logos/NCSU%20LOGO.jpg">
<ul id="navlist">
<li><a href="se.html">Home</a></li>
<li><a href="syllabus.html">Syllabus</a></li>
<li><a href="lectures.html">Lectures</a></li>
<li><a href="project.html">Project</a></li>
<li><a href="homeworks.html">Homeworks</a></li>
</ul>
</div>
<hr>
<em>Department of Computer Science, College of Engineering</em>
<div id="header">
<h1 class="title">Class Project</h1>
</div>
<div id="TOC">
<ul>
<li><a href="#summary-of-projects">Summary of Projects</a></li>
<li><a href="#project1">Project1</a><ul>
<li><a href="#tracks">Tracks</a></li>
<li><a href="#tracking">Tracking</a></li>
<li><a href="#about-the-tracks">About the Tracks</a><ul>
<li><a href="#technical-track">TEchnical Track</a></li>
<li><a href="#empirical-track">Empirical Track</a></li>
<li><a href="#management-track-thinking-beyond-the-state-of-the-art">Management Track: Thinking Beyond the State of the Art</a></li>
</ul></li>
</ul></li>
<li><a href="#project-2">Project 2</a></li>
</ul>
</div>
<h1 id="summary-of-projects"><a href="#summary-of-projects">Summary of Projects</a></h1>
<p>Project1: do something<br />Project2: reflect on what others did, observe &quot;bad smells&quot;, suggest improvements.</p>
<h1 id="project1"><a href="#project1">Project1</a></h1>
<h2 id="tracks"><a href="#tracks">Tracks</a></h2>
<ul>
<li>Empirical - Answer a research question using data.
<ul>
<li>Do this one if your team like data mining</li>
</ul></li>
<li>Technical - Replicate, extend, or build a new technical research result.
<ul>
<li>Do this one if your team is full of uber programming geeks</li>
</ul></li>
<li>Management - Apply new research results to practice.
<ul>
<li>Do this one if your team likes the strategic view</li>
</ul></li>
</ul>
<h2 id="tracking"><a href="#tracking">Tracking</a></h2>
<ul>
<li>For all this work, work Develop a software engineering research prototype and conduct preliminary evaluation of the developed prototype in Github.
<ul>
<li>Define a workflow, which is documented as GitHub labels.</li>
<li>Communicate within the team via issue reports.</li>
<li>Tag each issue with labels taken from the e interested in pursuing this option, please meet with the instructor on discussing the possible topics..</li>
</ul></li>
</ul>
<p>Note that Project2 will use these tracks to check for <em>bad smells</em>.</p>
<h2 id="about-the-tracks"><a href="#about-the-tracks">About the Tracks</a></h2>
<h3 id="technical-track"><a href="#technical-track">TEchnical Track</a></h3>
<p>Develop <strong>two</strong> software engineering research prototype and conduct preliminary evaluation of the developed prototypes. In your report, describe and compare and contrast the two approaches.</p>
<p>This project: only recommended for those with a strong background both in software engineering and in system implementation. If you are interested in pursuing this option, please meet with the instructor on discussing the possible topics.</p>
<p>Report + short video (5mins) describing the tools (posted to YouTube) and a link to code in a github repository.</p>
<h3 id="empirical-track"><a href="#empirical-track">Empirical Track</a></h3>
<ul>
<li>Using a dataset from github, http://openscience.us/repo, Stack Overflow, or other approved dataset.</li>
<li>Manually sample some data, form hypotheses and research questions.</li>
<li>Process, clean, parse, filter, relate, and analyze data.</li>
<li>Write report including figures, relevant statistics, etc.
<ul>
<li>Link to data processing code in a github repository</li>
</ul></li>
</ul>
<h3 id="management-track-thinking-beyond-the-state-of-the-art"><a href="#management-track-thinking-beyond-the-state-of-the-art">Management Track: Thinking Beyond the State of the Art</a></h3>
<p>Select 8 papers from <a href="http://dl.acm.org/citation.cfm?id=2568225&amp;CFID=470308131&amp;CFTOKEN=61523672">ICSE'14</a>, Technical Track, 2 max from NIER Track on a topic that interests you.</p>
<p>Write a survey paper that reviews the field around these papers: problems, approaches, contributions, related work; and connects all papers under some common themes. Write a management report that describes how a current practice can be improved, or should be avoided.</p>
<p>To find the context around the paper, <em>cluster</em> a <em>structured review</em>. In the following, step 4 is somewhat labor intensive but between 4 people can take less than a week.</p>
<ol style="list-style-type: decimal">
<li>Pick some topic in SE (hint, the more focused, the better).</li>
<li>Use domain knowledge to pick three high impact seed articles;</li>
<li>Used Google to find 500+ relevant studies that cited any of those seed articles (see http://scholar.google.com/);</li>
<li>Removed false positives by scanning titles and abstracts. This reduced the 500+ articles to less than one hundred;</li>
<li>Applied <em>relevancy rules</em> to reduce the papers to two or three doze e.g.
<ul>
<li>``reject all papers that do not offer a univariate predictive analysis for the validation of the metric(s) under investigation''.</li>
</ul></li>
<li>Checked the literature reviews of important papers in this field for papers not in our sample.</li>
</ol>
<p>For the remaining papers in the sample, perform a <a href="http://www.cs.rug.nl/~paris/papers/ICSE11.pdf">repertory grid analysis</a>. In summary, from the two to three dozen papers:</p>
<ul>
<li>Pick 3 papers at random</li>
<li>Ask which 2 are closest</li>
<li>Ask what most distinguishes those two from the third</li>
<li>Repeat until new trios of papers do not generate new constructs</li>
<li>Score and group the papers</li>
<li>Cluster the rows and columns using the techniques discussed in lectures</li>
</ul>
<center>
<img src="../img/repgrid.png">
</center>

<p>(These <em>clustered heat maps</em> can be <a href="http://www.r-bloggers.com/drawing-heatmaps-in-r/">generated in &quot;R&quot;</a>.)</p>
<p>Then :</p>
<ul>
<li>For each of the clusters found above, offer a strategic comment such as <em>this is good</em>, or <em>here, bad things happens</em>.</li>
<li>Make recommendations on how to avoid the bad and get to the good.</li>
<li>Comment on the <em>road not taken</em>; i.e. gaps between the clusters that no one is doing, but should.</li>
</ul>
<p>For examples of this kind of analysis, see <a href="http://www.cs.rug.nl/~paris/papers/ICSE11.pdf">Tofan et al</a> and <a href="http://agent-ready.googlecode.com/svn-history/r120/branches/andres/share/pdf/menzies02.pdf">Menzies et al</a> (but note that these do not do the initial mass queries to find a large corpus).</p>
<h1 id="project-2"><a href="#project-2">Project 2</a></h1>
<p>Starts in April. Details TBD. But it studies the workflows seen in Project1.</p>
<div class="references">

</div>
</body>
</html>
